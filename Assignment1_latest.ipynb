{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52fdab5a075c0b5b2c392d9f3e890a9c",
     "grade": false,
     "grade_id": "cell-e81de4401376cd82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "This is the first of two mandatory assignments which must be completed during the course. First some practical information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Sunday, August 19, 2018.**\n",
    "* How do you grade the assignment?: You will **peergrade** each other as primary grading. \n",
    "* Can i work with my group?: **yes**\n",
    "\n",
    "The assigment consist of one to tree problems from each of the exercise sets you have solved so far (excluding Set 1). We've tried to select problems which are self contained, but it might be nessecary to solve some of the previous exercises in each set to fully answer the problems in this assignment.\n",
    "\n",
    "## Problems from Exercise Set 2:\n",
    "\n",
    "> **Ex. 2.2**: Make two lists. The first should be numbered. The second should be unnumbered and contain at least one sublevel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Answer to Ex. 2.2 here] (convert to markdown cell)\n",
    "\n",
    "### First list\n",
    "\n",
    "1. Item number 1\n",
    "1. Item number 2\n",
    "1. Item number 3\n",
    "\n",
    "### Second list\n",
    "\n",
    "* Item number 1\n",
    "* Item number 2\n",
    "    * Item number 2a\n",
    "    * Item number 2b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfa16d57d526109fffe155cea5147bc",
     "grade": false,
     "grade_id": "cell-075bc22ce52a4ffe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from Exercise set 3:\n",
    "\n",
    "> **Ex. 3.1.3:** Let `l1 = ['r ', 'Is', '>', ' < ', 'g ', '?']`. Create from `l1` the sentence `\"Is r > g?\"` using your knowledge about string formatting. Store this new string in a variable called `answer_31`. Make sure there is only one space in between worlds.\n",
    ">\n",
    ">> _Hint:_ You should be able to combine the above informations to solve this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f945a62727e4c9616a38b1d1f6673d",
     "grade": false,
     "grade_id": "problem_31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.3 here]\n",
    "l1 = ['r ', 'Is', '>', ' < ', 'g ', '?']\n",
    "\n",
    "# answer_31 = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_31 = l1[1].strip() + ' ' + l1[0].strip() + ' ' + l1[2].strip() + ' ' + l1[4].strip() + l1[5].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "99da4cf8dc98518963d95fb054131224",
     "grade": true,
     "grade_id": "problem_31_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_31 == \"Is r > g?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc2c4cc2a1cdba681ed1006bba8f4619",
     "grade": false,
     "grade_id": "cell-1d737e504b202c4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 3.1.4**: Create an empty dictionary `words` using the `dict()`function. Then add each of the words in `['animal', 'coffee', 'python', 'unit', 'knowledge', 'tread', 'arise']` as a key, with the value being a boolean indicator for whether the word begins with a vowel. The results should look like `{'bacon': False, 'asynchronous': True ...}`. Store the result in a new variable called `answer_32`.\n",
    ">\n",
    ">> _Hint:_ You might want co first construct a function that asseses whether a given word begins with a vowel or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "77bbae13d6cb074fac5b05890acaf4e6",
     "grade": false,
     "grade_id": "problem_32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'animal': True, 'coffee': False, 'python': False, 'unit': True, 'knowledge': False, 'tread': False, 'arise': True}\n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 3.1.4 here]\n",
    "W = ['animal', 'coffee', 'python', 'unit', 'knowledge', 'tread', 'arise']\n",
    "\n",
    "# answer_32 = \n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def VowCheck (z):\n",
    "    if(z=='A' or z=='a' or z=='E' or z =='e' or z=='I' or z=='i' or z=='O' or z=='o' or z=='U' or z=='u'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "answer_32 = dict()\n",
    "\n",
    "keys = ['animal', 'coffee', 'python', 'unit', 'knowledge', 'tread', 'arise']\n",
    "values = [VowCheck(keys[0][0]), VowCheck(keys[1][0]), VowCheck(keys[2][0]), \n",
    "          VowCheck(keys[3][0]), VowCheck(keys[4][0]), VowCheck(keys[5][0]), VowCheck(keys[6][0])]\n",
    "key_value_pairs = list(zip(keys, values, ))\n",
    "\n",
    "answer_32.update (key_value_pairs)\n",
    "\n",
    "print(answer_32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9b444b7f9f76b5dc862d1aae155c794",
     "grade": true,
     "grade_id": "problem_32_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_32 == {i: i[0] in 'aeiou' for i in W}\n",
    "assert sorted(answer_32) == sorted(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "29d96b09d12a59e80fc8cc796d836363",
     "grade": false,
     "grade_id": "cell-b4f75d07ccec62ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 3.3.2:** use the `requests` module (get it with `pip install requests`) and `construct_link()` which you defined in the previous question (ex 3.3.1) to request birth data from the \"FOD\" table. Get all available years (variable \"Tid\"), but only female births (BARNKON=P) . Unpack the json payload and store the result. Wrap the whole thing in a function which takes an url as input and returns the corresponding output.\n",
    ">\n",
    "> Store the birth data in a new variable called `answer_33`.\n",
    ">\n",
    ">> _Hint:_ The `requests.response` object has a `.json()` method. \n",
    ">\n",
    ">> _Note:_ you wrote `construct_link()` in 3.3.1, if you didn't heres the link you need to get: `https://api.statbank.dk/v1/data/FOLK1A/JSONSTAT?lang=en&Tid=*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "68d07574aa272d282cfabe0adbd40035",
     "grade": false,
     "grade_id": "problem_33",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eb6cf3528aa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 3.3.2 here]\n",
    "\n",
    "# answer_33 = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05a6f9bbf5fc1d972e3bdfb90bf7d8c0",
     "grade": true,
     "grade_id": "problem_33_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert sorted(answer_33['dataset'].keys()) == ['dimension', 'label', 'source', 'updated', 'value']\n",
    "assert 'BARNKON' in answer_33['dataset']['dimension'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "080f814e193a79742f10b565f49e667b",
     "grade": false,
     "grade_id": "cell-7738e4de11c9ba89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc62d41fddcf5f9cc24531cbc8aeb995",
     "grade": false,
     "grade_id": "cell-6ce85387893ff77f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f4eb9718da6000d04736de9af469001",
     "grade": false,
     "grade_id": "cell-0cf565ee06bf9606",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 4.1.1:** Use Pandas' CSV reader to fetch  daily data weather from 1864 for various stations - available [here](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/). Store the dataframe in a variable called `answer_41`.\n",
    ">\n",
    ">> *Hint 1*: for compressed files you may need to specify the keyword `compression`.\n",
    ">\n",
    ">> *Hint 2*: keyword `header` can be specified as the CSV has no column names.\n",
    ">\n",
    ">> *Hint 3*: Specify the path, as the URL linking directly to the 1864 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c5b74ef23beb0bc773344af6e8e90809",
     "grade": false,
     "grade_id": "problem_41",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 4.1.1 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "path_1864 = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1864.csv.gz'\n",
    "\n",
    "answer_41 = pd.read_csv(path_1864, compression = 'gzip', header = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "238d18588151c3bdb43bce91f5f5c908",
     "grade": true,
     "grade_id": "problem_41_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_41.shape == (27349, 8)\n",
    "assert list(answer_41.columns) == list(range(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c9865188a9e267f6ae311090805d3c6",
     "grade": false,
     "grade_id": "cell-fef3fd772e3b2e19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 4.1.2:** Structure your weather DataFrame by using only the relevant columns (station identifier, data, observation type, observation value), rename them. Make sure observations are correctly formated (how many decimals should we add? one?).\n",
    ">\n",
    "> Store the resulting dataframe in a new variable called `answer_42`.\n",
    ">\n",
    ">> *Hint:* rename can be done with `df.columns=COLS` where `COLS` is a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f8b0be8cbe49efa6f85b2a318effdf31",
     "grade": false,
     "grade_id": "problem_42",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 4.1.2 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_42 = answer_41.drop(columns = [4, 5,6, 7], axis = 1)\n",
    "\n",
    "answer_42 = answer_42.rename(columns = {0: 'Station Identifier', 1: 'Date', 2: 'Observation type', 3: 'Observation value'})\n",
    "\n",
    "answer_42['Observation value'] = answer_42['Observation value']/10\n",
    "#One decimal is efficient\n",
    "\n",
    "print(answer_42.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5559217edb771ff89b82031f801c8eee",
     "grade": true,
     "grade_id": "problem_42_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_42.shape == (27349, 4)\n",
    "assert 144.8 in [answer_42[i].max() for i in answer_42]\n",
    "assert -666.0 in [answer_42[i].min() for i in answer_42]\n",
    "assert 18640101 in [answer_42[i].min() for i in answer_42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "003c5b1c802d4d1556083d23611093ad",
     "grade": false,
     "grade_id": "cell-323f47f2db7307c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 4.1.3:**  Select data for the station `ITE00100550` and only observations for maximal temperature. Make a copy of the DataFrame. Explain in a one or two sentences how copying works.\n",
    ">\n",
    "> Store the subsetted dataframe in a new variable called `answer_43`.\n",
    ">\n",
    ">> *Hint 1*: the `&` operator works elementwise on boolean series (like `and` in core python).\n",
    ">\n",
    ">> *Hint 2*: copying of the dataframe is done with the `copy` method for DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35bba693b17ba982b78783c2e510c1be",
     "grade": false,
     "grade_id": "problem_43",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 4.1.3 here]\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_43 = answer_42[answer_42['Station Identifier'].str.contains(\"ITE00100550\") & answer_42['Observation type'].str.contains(\"TMAX\")]\n",
    "\n",
    "print(answer_43.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db7520577f35003b0ce18b42f0d6bd2e",
     "grade": true,
     "grade_id": "problem_43_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'ITE00100550' in [answer_43[i].min() for i in answer_43]\n",
    "assert 'ITE00100550' in [answer_43[i].max() for i in answer_43]\n",
    "assert 'TMAX' in [answer_43[i].min() for i in answer_43]\n",
    "assert 'TMAX' in [answer_43[i].max() for i in answer_43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40769f7d8f4a1b396187443bc14517d9",
     "grade": false,
     "grade_id": "cell-d6100b901a5d4ba8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 4.1.4:** Make a new column in `answer_44` called `TMAX_F` where you have converted the temperature variables to Fahrenheit. Make sure not to overwrite `answer_43`.\n",
    ">\n",
    "> Store the resulting dataframe in a variable called `answer_44`.\n",
    ">\n",
    ">> *Hint*: Conversion is $F = 32 + 1.8*C$ where $F$ is Fahrenheit and $C$ is Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e88dde4ccdbddb981100b636b0d1d448",
     "grade": false,
     "grade_id": "problem_44",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 4.1.4 here]\n",
    "answer_44 = answer_43.copy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_44['TMAX_F'] = 32 + 1.8 * answer_44['Observation value']\n",
    "\n",
    "print(answer_44.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cb8fb435c837f57fc91f5d55df8709c7",
     "grade": true,
     "grade_id": "problem_44_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(answer_44.columns) -  set(answer_43.columns) == {'TMAX_F'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7abe66bdec5e0922773485287403a31f",
     "grade": false,
     "grade_id": "cell-69a5ddb907fc34bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ec8e5eab14daeb68b706c206bce27a4",
     "grade": false,
     "grade_id": "cell-dc072f65e62126e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44db0d95253cc709f8289a00c40bd014",
     "grade": false,
     "grade_id": "cell-ef0604c76e360580",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 5.1.1:**: Show the first five rows of the titanic dataset. What information is in the dataset? Use a barplot to show the probability of survival for men and women within each passenger class. Can you make a boxplot showing the same information (why/why not?). _Bonus:_ show a boxplot for the fare-prices within each passenger class. \n",
    ">\n",
    "> Spend five minutes discussing what you can learn about the survival-selection aboard titanic from the figure(s).\n",
    ">\n",
    "> > _Hint:_ https://seaborn.pydata.org/generated/seaborn.barplot.html, specifically the `hue` option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "18ef2e9eff41ceb6b26fb588206b684f",
     "grade": true,
     "grade_id": "problem_51",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 5.1.1 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(titanic.head(5))\n",
    "\n",
    "sns.barplot(x = titanic['class'], y = titanic['survived'], hue = titanic['sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = sns.boxplot(x='class', y='fare', hue='sex', data=titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc466900f77bd9400665fc24523ba7d7",
     "grade": false,
     "grade_id": "cell-8d78955bde3112ba",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 5.1.2:** Using the iris flower dataset, draw a scatterplot of sepal length and petal length. Include a second order polynomial fitted to the data. Add a title to the plot and rename the axis labels.\n",
    "> _Discuss:_ Is this a meaningful way to display the data? What could we do differently?\n",
    ">\n",
    "> For a better understanding of the dataset this image might be useful:\n",
    "> <img src=\"iris_pic.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    ">\n",
    ">> _Hint:_ use the `.regplot` method from seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdb3b79fb6852431c37310696e841913",
     "grade": true,
     "grade_id": "cell-8631e73e3796008b",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 5.1.2 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(iris.head(5))\n",
    "\n",
    "iris_plot = sns.regplot('petal_length', 'sepal_length', data = iris, scatter = True, fit_reg = True, order = 2)\n",
    "\n",
    "iris_plot.set(xlabel = 'petal', ylabel = 'sepal')\n",
    "iris_plot.set_title('Iris')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6955bf905fc3c63c64ac4420d11d1f6",
     "grade": false,
     "grade_id": "cell-95796f67165019ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 5.1.3:** Combine the two of the figures you created above into a two-panel figure similar to the one shown here:\n",
    "> <img src=\"Example.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    ">\n",
    "> Save the figure as a png file on your computer. \n",
    ">> _Hint:_ See [this question](https://stackoverflow.com/questions/41384040/subplot-for-seaborn-boxplot) on stackoverflow for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5552793303f021420e75daf3f7caa237",
     "grade": true,
     "grade_id": "cell-686e1fcbc475435a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 5.1.3 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "sns.regplot('petal_length', 'sepal_length', data = iris, scatter = True, fit_reg = True, order = 2, ax = axes[0])\n",
    "ax1 = sns.barplot(x = titanic['class'], y = titanic['survived'], hue = titanic['sex'], ax = axes[1])\n",
    "\n",
    "ax1.set_ylim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d4010a7acb4875fd0440dbc6057bb84a",
     "grade": false,
     "grade_id": "cell-e280df998b60ae2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 5.1.4:** Use [pairplot with hue](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to create a figure that clearly shows how the different species vary across measurements. Change the color palette and remove the shading from the density plots. _Bonus:_ Try to explain how the `diag_kws` argument works (_hint:_ [read here](https://stackoverflow.com/questions/1769403/understanding-kwargs-in-python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "492a7bf33f9d310739162fae57556a1f",
     "grade": true,
     "grade_id": "cell-0f5793c056a02040",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 5.1.4 here]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ax_514 = sns.pairplot(iris, hue = \"species\", palette = \"husl\",  diag_kws=dict(shade=False))\n",
    "ax_514\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50981ef2a4bf0b5c3c2c69baea6a7685",
     "grade": false,
     "grade_id": "cell-ee43396c59c5301f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 6\n",
    "\n",
    "> _Note:_ In the exercises we asked you to download weather data from the NOAA website. For this assignment the data are loaded in the following code cell into two pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06644b5719b20405b4baee66e20fe061",
     "grade": false,
     "grade_id": "cell-834b03b8b15b0307",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'weather_data_1864.csv' does not exist: b'weather_data_1864.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7db8f67fa175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mweather_1864\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weather_data_1864.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'weather_data_1864.csv' does not exist: b'weather_data_1864.csv'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weather_1864 = pd.read_csv('weather_data_1864.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b31c95426614b2adb9069483b538734e",
     "grade": false,
     "grade_id": "cell-3b947e0bc3564fc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 6.1.4:** Extract the country code from the station name into a separate column.\n",
    ">\n",
    "> Create a new column in `weather_1864` called `answer_61` and store the country codes here.\n",
    ">\n",
    ">> _Hint:_ The station column contains a GHCND ID, given to each weather station by NOAA. The format of these ID's is a 2-3 letter country code, followed by a integer identifying the specific station. A simple approach is to assume a fixed length of the country ID. A more complex way would be to use the [`re`](https://docs.python.org/2/library/re.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "449adcf52422faf0dec2cd826ff87d2a",
     "grade": false,
     "grade_id": "problem_61",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 6.1.4]\n",
    "# weather_1864['answer_61'] =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "path_1864 = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1864.csv.gz'\n",
    "\n",
    "df_csv = pd.read_csv(path_1864, compression = 'gzip', header = None)\n",
    "\n",
    "df_csv = df_csv.drop(columns = [4, 5,6, 7], axis = 1)\n",
    "\n",
    "df_csv = df_csv.rename(columns = {0: 'Station Identifier', 1: 'Date', 2: 'Observation type', 3: 'Observation value'})\n",
    "\n",
    "df_csv['answer_61'] = df_csv['Station Identifier'].str[0:2]\n",
    "\n",
    "df_csv['Datetime'] = df_csv['Date']\n",
    "\n",
    "df_csv['Datetime'] = df_csv['Datetime'].astype(str)\n",
    "\n",
    "df_csv['Datetime'] = pd.to_datetime(df_csv['Datetime'])\n",
    "\n",
    "df_csv['month'] = pd.DatetimeIndex(df_csv['Datetime']).month\n",
    "\n",
    "df_csv_TMAX = df_csv[df_csv['Observation type'].str.contains(\"TMAX\")]\n",
    "\n",
    "weather_1864 = df_csv_TMAX.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weather_1864.head())\n",
    "\n",
    "sorted(weather_1864['answer_61'].str[:2].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "149a5ad93fbe65bcda27008f62ebc94a",
     "grade": true,
     "grade_id": "problem_61_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather_1864' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5b00cb64306a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweather_1864\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'answer_61'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SZ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'EZ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UK'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'EI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'weather_1864' is not defined"
     ]
    }
   ],
   "source": [
    "assert sorted(weather_1864['answer_61'].str[:2].unique()) == sorted(['SZ', 'CA', 'EZ', 'GM', 'AU', 'IT', 'BE', 'UK', 'EI', 'AG', 'AS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d720993d20e02b6684611da9488872ea",
     "grade": false,
     "grade_id": "cell-369017dafb1e6d7c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 6.1.5:** Make a function that downloads and formats the weather data according to previous exercises in Exercise Section 4.1, 6.1. You should use data for ALL stations but still only select maximal temperature. _Bonus:_ To validate that your function works plot the temperature curve for each country in the same window. Use `plt.legend()` to add a legend. \n",
    ">\n",
    "> Name your function `prepareWeatherData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "920bf57a90b7599b51659c580eb9d604",
     "grade": false,
     "grade_id": "problem_62",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 6.1.5]\n",
    "\n",
    "#def prepareWeatherData(year):\n",
    "    # Your code here\n",
    "  #  return \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def prepareWeatherData(year):\n",
    "    df = pd.read_csv('https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/'+ year +'.csv.gz', compression='gzip', header=None, sep=',', quotechar='\"')\n",
    "    df_dropped = df.drop(columns=[4, 5, 6, 7])\n",
    "    df_renamed = df_dropped.rename(columns={0: \"Station identifier\", 1: \"Date\", 2: \"Observation type\", 3: \"Observation value\"})\n",
    "    df_reduced = df_renamed[df_renamed['Observation type'].str.contains(\"TMAX\")]\n",
    "    df_reduced['Fahrenheit'] = df_reduced['Observation value'] * 1.8 + 32\n",
    "    df_sorted = df_reduced.sort_values(by=['Observation value'])\n",
    "    df_reindexed = df_sorted.reset_index(drop=True)\n",
    "    df_string = df_reindexed.astype(str)\n",
    "    df_string['date_time'] = pd.to_datetime(df_string['Date'])\n",
    "    df_string['Observation value'] = pd.to_numeric(df_string['Observation value'])/10\n",
    "    df_string['dt_Month'] = pd.DatetimeIndex(df_string['date_time']).month\n",
    "    df_string['Country code'] = df_string['Station identifier'].str.replace('\\d+', '')\n",
    "    df_string = df_string.drop(columns=['Date'])\n",
    "    df_string = df_string.set_index('date_time')\n",
    "    return(df_string)\n",
    "\n",
    "prepareWeatherData('1864')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, figsize = (16, 9))\n",
    "\n",
    "prepareWeatherData('1864').groupby('Country code')['Observation value'].plot(legend=True)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21b573bbe818e27e3cba31ba439f889d",
     "grade": true,
     "grade_id": "problem_62_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert prepareWeatherData('1864').shape == (5686, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0a38a1a62cf78e9c6214e21ea8e6174",
     "grade": false,
     "grade_id": "cell-e206cc10e0354153",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 7\n",
    "\n",
    "> _Note:_ Once again if you haven't managed to download the data from NOAA, you can refer to the github repo to get csv-files containing the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aae2b85bce7a1f050aba06e5f14f07c8",
     "grade": false,
     "grade_id": "cell-15d553fb5be81d7a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Increases the plot size a little\n",
    "mpl.rcParams['figure.figsize'] = 11, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "807c2181bc38762b44c6706d86291424",
     "grade": false,
     "grade_id": "cell-c48aef6db605ecd8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 7.1.1:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for our station with the ID _'ITE00100550'_ in 1864. \n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ecfe6ce83747d19c96ac19778351daee",
     "grade": true,
     "grade_id": "problem_71",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.1]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df711 = prepareWeatherData('1864')[prepareWeatherData('1864')['Station identifier'].str.contains(\"ITE00100550\")]\n",
    "\n",
    "df_g = (df711['Observation value'].groupby(df711['dt_Month']).describe())\n",
    "df_g = df_g.loc [ :, ['max', 'mean', 'min', '25%', '75%']]\n",
    "\n",
    "print(df_g.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "210cb4ca3448895862b65daa1a32b1f1",
     "grade": false,
     "grade_id": "cell-0e3079a5520618d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 7.1.2:** Get the processed data from years 1864-1867 as a list of DataFrames. Convert the list into a single DataFrame by concatenating vertically. \n",
    ">\n",
    "> Name the concatenated data `answer_72`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b221fc2dbd44eb50ad781368243b9a9c",
     "grade": false,
     "grade_id": "problem_72",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.2]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "answer_72 = []\n",
    "\n",
    "for i in range(1864, 1868):\n",
    "    path_ = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/'+ str(i) +'.csv.gz'\n",
    "    df_csvloop = pd.read_csv(path_, compression = 'gzip', header = None)\n",
    "    df_csvloop = df_csvloop.drop(columns = [4, 5,6, 7], axis = 1)\n",
    "    df_csvloop = df_csvloop.rename(columns = {0: 'Station Identifier', 1: 'Date', 2: 'Observation type', 3: 'Observation value'})\n",
    "    df_csvloop['Country Code'] = df_csvloop['Station Identifier'].str.extract(r'([A-z]+)') #Use of Regex\n",
    "    df_csvloop['Datetime'] = df_csvloop['Date']\n",
    "    df_csvloop['Datetime'] = df_csvloop['Datetime'].astype(str)\n",
    "    df_csvloop['Datetime'] = pd.to_datetime(df_csvloop['Datetime'])\n",
    "    df_csvloop['month'] = pd.DatetimeIndex(df_csvloop['Datetime']).month\n",
    "    df_csvloop = df_csvloop[df_csvloop['Observation type'].str.contains(\"TMAX\")]\n",
    "    answer_72.append(df_csvloop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Station Identifier      Date Observation type  Observation value  \\\n",
      "0            ITE00100550  18640101             TMAX                 10   \n",
      "8            ASN00086071  18640101             TMAX                214   \n",
      "15           CA006158350  18640101             TMAX                 11   \n",
      "24           EIE00101859  18640101             TMAX                 94   \n",
      "28           BE000006447  18640101             TMAX                -23   \n",
      "34           GME00125218  18640101             TMAX                -81   \n",
      "39           EZE00100082  18640101             TMAX                -92   \n",
      "42           ITE00100554  18640101             TMAX                 17   \n",
      "47           ASN00066062  18640101             TMAX                244   \n",
      "53           AU000005901  18640101             TMAX                -50   \n",
      "55           GM000004204  18640101             TMAX                -68   \n",
      "59           UK000047811  18640101             TMAX                 59   \n",
      "67           UK000056225  18640101             TMAX                  5   \n",
      "72           SZ000006717  18640101             TMAX                -82   \n",
      "75           ITE00100550  18640102             TMAX                  8   \n",
      "83           ASN00086071  18640102             TMAX                234   \n",
      "90           CA006158350  18640102             TMAX               -161   \n",
      "99           EIE00101859  18640102             TMAX                 78   \n",
      "103          BE000006447  18640102             TMAX                -25   \n",
      "105          AGE00135039  18640102             TMAX                140   \n",
      "111          GME00125218  18640102             TMAX                -71   \n",
      "116          EZE00100082  18640102             TMAX                -98   \n",
      "119          ITE00100554  18640102             TMAX                 -3   \n",
      "124          ASN00066062  18640102             TMAX                266   \n",
      "130          AU000005901  18640102             TMAX                -70   \n",
      "132          GM000004204  18640102             TMAX                -63   \n",
      "136          UK000047811  18640102             TMAX                 42   \n",
      "144          UK000056225  18640102             TMAX                 -1   \n",
      "149          SZ000006717  18640102             TMAX               -107   \n",
      "152          ITE00100550  18640103             TMAX                -28   \n",
      "...                  ...       ...              ...                ...   \n",
      "47697        UK000056225  18671230             TMAX                 -6   \n",
      "47703        SZ000006717  18671230             TMAX               -115   \n",
      "47707        ITE00100550  18671231             TMAX                  4   \n",
      "47715        CA006150689  18671231             TMAX                -44   \n",
      "47721        ASN00086071  18671231             TMAX                218   \n",
      "47729        CA006158350  18671231             TMAX                 -6   \n",
      "47740        EIE00101859  18671231             TMAX                  6   \n",
      "47743        CA006153192  18671231             TMAX                -17   \n",
      "47747        CA006139520  18671231             TMAX                  6   \n",
      "47751        CA006110549  18671231             TMAX                -17   \n",
      "47757        BE000006447  18671231             TMAX                -39   \n",
      "47763        EI000003969  18671231             TMAX                 34   \n",
      "47766        GME00125218  18671231             TMAX                -88   \n",
      "47772        EZE00100082  18671231             TMAX                -75   \n",
      "47775        CA006166416  18671231             TMAX                -56   \n",
      "47779        CA006106362  18671231             TMAX               -111   \n",
      "47783        ITE00100554  18671231             TMAX                  6   \n",
      "47790        ASN00066062  18671231             TMAX                293   \n",
      "47797        AU000005901  18671231             TMAX               -104   \n",
      "47799        GM000004204  18671231             TMAX                -63   \n",
      "47807        UK000047811  18671231             TMAX                 28   \n",
      "47810        USC00288878  18671231             TMAX                -22   \n",
      "47818        CA006122845  18671231             TMAX                 -6   \n",
      "47824        CA006148100  18671231             TMAX                -33   \n",
      "47828        ASN00074128  18671231             TMAX                222   \n",
      "47831        CA006101872  18671231             TMAX                -39   \n",
      "47835        CA006137735  18671231             TMAX                -11   \n",
      "47839        ASN00090015  18671231             TMAX                167   \n",
      "47840        UK000056225  18671231             TMAX                 -5   \n",
      "47846        SZ000006717  18671231             TMAX               -158   \n",
      "\n",
      "      Country Code   Datetime  month  \n",
      "0              ITE 1864-01-01      1  \n",
      "8              ASN 1864-01-01      1  \n",
      "15              CA 1864-01-01      1  \n",
      "24             EIE 1864-01-01      1  \n",
      "28              BE 1864-01-01      1  \n",
      "34             GME 1864-01-01      1  \n",
      "39             EZE 1864-01-01      1  \n",
      "42             ITE 1864-01-01      1  \n",
      "47             ASN 1864-01-01      1  \n",
      "53              AU 1864-01-01      1  \n",
      "55              GM 1864-01-01      1  \n",
      "59              UK 1864-01-01      1  \n",
      "67              UK 1864-01-01      1  \n",
      "72              SZ 1864-01-01      1  \n",
      "75             ITE 1864-01-02      1  \n",
      "83             ASN 1864-01-02      1  \n",
      "90              CA 1864-01-02      1  \n",
      "99             EIE 1864-01-02      1  \n",
      "103             BE 1864-01-02      1  \n",
      "105            AGE 1864-01-02      1  \n",
      "111            GME 1864-01-02      1  \n",
      "116            EZE 1864-01-02      1  \n",
      "119            ITE 1864-01-02      1  \n",
      "124            ASN 1864-01-02      1  \n",
      "130             AU 1864-01-02      1  \n",
      "132             GM 1864-01-02      1  \n",
      "136             UK 1864-01-02      1  \n",
      "144             UK 1864-01-02      1  \n",
      "149             SZ 1864-01-02      1  \n",
      "152            ITE 1864-01-03      1  \n",
      "...            ...        ...    ...  \n",
      "47697           UK 1867-12-30     12  \n",
      "47703           SZ 1867-12-30     12  \n",
      "47707          ITE 1867-12-31     12  \n",
      "47715           CA 1867-12-31     12  \n",
      "47721          ASN 1867-12-31     12  \n",
      "47729           CA 1867-12-31     12  \n",
      "47740          EIE 1867-12-31     12  \n",
      "47743           CA 1867-12-31     12  \n",
      "47747           CA 1867-12-31     12  \n",
      "47751           CA 1867-12-31     12  \n",
      "47757           BE 1867-12-31     12  \n",
      "47763           EI 1867-12-31     12  \n",
      "47766          GME 1867-12-31     12  \n",
      "47772          EZE 1867-12-31     12  \n",
      "47775           CA 1867-12-31     12  \n",
      "47779           CA 1867-12-31     12  \n",
      "47783          ITE 1867-12-31     12  \n",
      "47790          ASN 1867-12-31     12  \n",
      "47797           AU 1867-12-31     12  \n",
      "47799           GM 1867-12-31     12  \n",
      "47807           UK 1867-12-31     12  \n",
      "47810          USC 1867-12-31     12  \n",
      "47818           CA 1867-12-31     12  \n",
      "47824           CA 1867-12-31     12  \n",
      "47828          ASN 1867-12-31     12  \n",
      "47831           CA 1867-12-31     12  \n",
      "47835           CA 1867-12-31     12  \n",
      "47839          ASN 1867-12-31     12  \n",
      "47840           UK 1867-12-31     12  \n",
      "47846           SZ 1867-12-31     12  \n",
      "\n",
      "[30003 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "answer_72 = pd.concat(answer_72, axis = 0, sort = False)\n",
    "\n",
    "print(answer_72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "107cafe2715660c3c36cb1ffab1e7fd6",
     "grade": true,
     "grade_id": "problem_72_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_72.shape == (30003, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a8926f4bb982bcc276dc7e8b5f1c3e0",
     "grade": false,
     "grade_id": "cell-5b9d68d7b6583e3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 7.1.3:** Parse the station location data which you can find at https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt. Merge station locations onto the weather data spanning 1864-1867.  \n",
    ">\n",
    "> Store the merged data in a new variable called `answer_73`.\n",
    ">\n",
    "> _Hint:_ The location data have the folllowing format, \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "> *Hint*: The station information has fixed width format - does there exist a pandas reader for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "statlocpath = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "\n",
    "path_2 = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1864.csv.gz'\n",
    "df_csvloop2 = pd.read_csv(path_2, compression = 'gzip', header = None)\n",
    "df_csvloop2 = df_csvloop2.drop(columns = [4, 5,6, 7], axis = 1)\n",
    "df_csvloop2 = df_csvloop2.rename(columns = {0: 'Station Identifier', 1: 'Date', 2: 'Observation type', 3: 'Observation value'})\n",
    "df_csvloop2['Country Code'] = df_csvloop2['Station Identifier'].str.extract(r'([A-z]+)') #Use of Regex\n",
    "df_csvloop2['Datetime'] = df_csvloop2['Date']\n",
    "df_csvloop2['Datetime'] = df_csvloop2['Datetime'].astype(str)\n",
    "df_csvloop2['Datetime'] = pd.to_datetime(df_csvloop2['Datetime'])\n",
    "df_csvloop2['month'] = pd.DatetimeIndex(df_csvloop2['Datetime']).month\n",
    "df_csvloop2 = df_csvloop2[df_csvloop2['Observation type'].str.contains(\"TMAX\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "167a6bcc96b0b283f04e3506dffefedb",
     "grade": false,
     "grade_id": "problem_73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.3]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "columns = [(1,11), (13,20), (22,30), (32,37), (38,40), (42,71), (73,75), (77,79), (81,82)]\n",
    "\n",
    "df_merged = pd.read_fwf(statlocpath, header = None, colspecs = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Station Identifier      Date Observation type  Observation value  \\\n",
      "0            ITE00100550  18640101             TMAX                 10   \n",
      "8            ASN00086071  18640101             TMAX                214   \n",
      "15           CA006158350  18640101             TMAX                 11   \n",
      "24           EIE00101859  18640101             TMAX                 94   \n",
      "28           BE000006447  18640101             TMAX                -23   \n",
      "34           GME00125218  18640101             TMAX                -81   \n",
      "39           EZE00100082  18640101             TMAX                -92   \n",
      "42           ITE00100554  18640101             TMAX                 17   \n",
      "47           ASN00066062  18640101             TMAX                244   \n",
      "53           AU000005901  18640101             TMAX                -50   \n",
      "55           GM000004204  18640101             TMAX                -68   \n",
      "59           UK000047811  18640101             TMAX                 59   \n",
      "67           UK000056225  18640101             TMAX                  5   \n",
      "72           SZ000006717  18640101             TMAX                -82   \n",
      "75           ITE00100550  18640102             TMAX                  8   \n",
      "83           ASN00086071  18640102             TMAX                234   \n",
      "90           CA006158350  18640102             TMAX               -161   \n",
      "99           EIE00101859  18640102             TMAX                 78   \n",
      "103          BE000006447  18640102             TMAX                -25   \n",
      "105          AGE00135039  18640102             TMAX                140   \n",
      "111          GME00125218  18640102             TMAX                -71   \n",
      "116          EZE00100082  18640102             TMAX                -98   \n",
      "119          ITE00100554  18640102             TMAX                 -3   \n",
      "124          ASN00066062  18640102             TMAX                266   \n",
      "130          AU000005901  18640102             TMAX                -70   \n",
      "132          GM000004204  18640102             TMAX                -63   \n",
      "136          UK000047811  18640102             TMAX                 42   \n",
      "144          UK000056225  18640102             TMAX                 -1   \n",
      "149          SZ000006717  18640102             TMAX               -107   \n",
      "152          ITE00100550  18640103             TMAX                -28   \n",
      "...                  ...       ...              ...                ...   \n",
      "27209        CA006158350  18641230             TMAX                -11   \n",
      "27217        EIE00101859  18641230             TMAX                 72   \n",
      "27221        BE000006447  18641230             TMAX                -21   \n",
      "27223        AGE00135039  18641230             TMAX                120   \n",
      "27230        GME00125218  18641230             TMAX                -16   \n",
      "27235        EZE00100082  18641230             TMAX                -73   \n",
      "27238        ITE00100554  18641230             TMAX                 57   \n",
      "27242        ASN00066062  18641230             TMAX                219   \n",
      "27248        AU000005901  18641230             TMAX                -10   \n",
      "27250        GM000004204  18641230             TMAX                -33   \n",
      "27254        UK000047811  18641230             TMAX                 73   \n",
      "27263        ASN00090015  18641230             TMAX                211   \n",
      "27265        UK000056225  18641230             TMAX                 17   \n",
      "27270        SZ000006717  18641230             TMAX                -40   \n",
      "27272        ITE00100550  18641231             TMAX                 33   \n",
      "27280        ASN00086071  18641231             TMAX                234   \n",
      "27286        CA006158350  18641231             TMAX                -61   \n",
      "27294        EIE00101859  18641231             TMAX                 72   \n",
      "27298        BE000006447  18641231             TMAX                -16   \n",
      "27300        AGE00135039  18641231             TMAX                145   \n",
      "27307        GME00125218  18641231             TMAX                -30   \n",
      "27312        EZE00100082  18641231             TMAX                -68   \n",
      "27315        ITE00100554  18641231             TMAX                 15   \n",
      "27319        ASN00066062  18641231             TMAX                223   \n",
      "27325        AU000005901  18641231             TMAX                -28   \n",
      "27327        GM000004204  18641231             TMAX                -34   \n",
      "27331        UK000047811  18641231             TMAX                 40   \n",
      "27340        ASN00090015  18641231             TMAX                239   \n",
      "27342        UK000056225  18641231             TMAX                 16   \n",
      "27347        SZ000006717  18641231             TMAX                -62   \n",
      "\n",
      "      Country Code   Datetime  month  \n",
      "0              ITE 1864-01-01      1  \n",
      "8              ASN 1864-01-01      1  \n",
      "15              CA 1864-01-01      1  \n",
      "24             EIE 1864-01-01      1  \n",
      "28              BE 1864-01-01      1  \n",
      "34             GME 1864-01-01      1  \n",
      "39             EZE 1864-01-01      1  \n",
      "42             ITE 1864-01-01      1  \n",
      "47             ASN 1864-01-01      1  \n",
      "53              AU 1864-01-01      1  \n",
      "55              GM 1864-01-01      1  \n",
      "59              UK 1864-01-01      1  \n",
      "67              UK 1864-01-01      1  \n",
      "72              SZ 1864-01-01      1  \n",
      "75             ITE 1864-01-02      1  \n",
      "83             ASN 1864-01-02      1  \n",
      "90              CA 1864-01-02      1  \n",
      "99             EIE 1864-01-02      1  \n",
      "103             BE 1864-01-02      1  \n",
      "105            AGE 1864-01-02      1  \n",
      "111            GME 1864-01-02      1  \n",
      "116            EZE 1864-01-02      1  \n",
      "119            ITE 1864-01-02      1  \n",
      "124            ASN 1864-01-02      1  \n",
      "130             AU 1864-01-02      1  \n",
      "132             GM 1864-01-02      1  \n",
      "136             UK 1864-01-02      1  \n",
      "144             UK 1864-01-02      1  \n",
      "149             SZ 1864-01-02      1  \n",
      "152            ITE 1864-01-03      1  \n",
      "...            ...        ...    ...  \n",
      "27209           CA 1864-12-30     12  \n",
      "27217          EIE 1864-12-30     12  \n",
      "27221           BE 1864-12-30     12  \n",
      "27223          AGE 1864-12-30     12  \n",
      "27230          GME 1864-12-30     12  \n",
      "27235          EZE 1864-12-30     12  \n",
      "27238          ITE 1864-12-30     12  \n",
      "27242          ASN 1864-12-30     12  \n",
      "27248           AU 1864-12-30     12  \n",
      "27250           GM 1864-12-30     12  \n",
      "27254           UK 1864-12-30     12  \n",
      "27263          ASN 1864-12-30     12  \n",
      "27265           UK 1864-12-30     12  \n",
      "27270           SZ 1864-12-30     12  \n",
      "27272          ITE 1864-12-31     12  \n",
      "27280          ASN 1864-12-31     12  \n",
      "27286           CA 1864-12-31     12  \n",
      "27294          EIE 1864-12-31     12  \n",
      "27298           BE 1864-12-31     12  \n",
      "27300          AGE 1864-12-31     12  \n",
      "27307          GME 1864-12-31     12  \n",
      "27312          EZE 1864-12-31     12  \n",
      "27315          ITE 1864-12-31     12  \n",
      "27319          ASN 1864-12-31     12  \n",
      "27325           AU 1864-12-31     12  \n",
      "27327           GM 1864-12-31     12  \n",
      "27331           UK 1864-12-31     12  \n",
      "27340          ASN 1864-12-31     12  \n",
      "27342           UK 1864-12-31     12  \n",
      "27347           SZ 1864-12-31     12  \n",
      "\n",
      "[5686 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_csvloop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Station Identifier      Date Observation type  Observation value  \\\n",
      "0           ITE00100550  18640101             TMAX                 10   \n",
      "1           ASN00086071  18640101             TMAX                214   \n",
      "2           CA006158350  18640101             TMAX                 11   \n",
      "3           EIE00101859  18640101             TMAX                 94   \n",
      "4           BE000006447  18640101             TMAX                -23   \n",
      "5           GME00125218  18640101             TMAX                -81   \n",
      "6           EZE00100082  18640101             TMAX                -92   \n",
      "7           ITE00100554  18640101             TMAX                 17   \n",
      "8           ASN00066062  18640101             TMAX                244   \n",
      "9           AU000005901  18640101             TMAX                -50   \n",
      "10          GM000004204  18640101             TMAX                -68   \n",
      "11          UK000047811  18640101             TMAX                 59   \n",
      "12          UK000056225  18640101             TMAX                  5   \n",
      "13          SZ000006717  18640101             TMAX                -82   \n",
      "14          ITE00100550  18640102             TMAX                  8   \n",
      "15          ASN00086071  18640102             TMAX                234   \n",
      "16          CA006158350  18640102             TMAX               -161   \n",
      "17          EIE00101859  18640102             TMAX                 78   \n",
      "18          BE000006447  18640102             TMAX                -25   \n",
      "19          AGE00135039  18640102             TMAX                140   \n",
      "20          GME00125218  18640102             TMAX                -71   \n",
      "21          EZE00100082  18640102             TMAX                -98   \n",
      "22          ITE00100554  18640102             TMAX                 -3   \n",
      "23          ASN00066062  18640102             TMAX                266   \n",
      "24          AU000005901  18640102             TMAX                -70   \n",
      "25          GM000004204  18640102             TMAX                -63   \n",
      "26          UK000047811  18640102             TMAX                 42   \n",
      "27          UK000056225  18640102             TMAX                 -1   \n",
      "28          SZ000006717  18640102             TMAX               -107   \n",
      "29          ITE00100550  18640103             TMAX                -28   \n",
      "...                 ...       ...              ...                ...   \n",
      "5656        CA006158350  18641230             TMAX                -11   \n",
      "5657        EIE00101859  18641230             TMAX                 72   \n",
      "5658        BE000006447  18641230             TMAX                -21   \n",
      "5659        AGE00135039  18641230             TMAX                120   \n",
      "5660        GME00125218  18641230             TMAX                -16   \n",
      "5661        EZE00100082  18641230             TMAX                -73   \n",
      "5662        ITE00100554  18641230             TMAX                 57   \n",
      "5663        ASN00066062  18641230             TMAX                219   \n",
      "5664        AU000005901  18641230             TMAX                -10   \n",
      "5665        GM000004204  18641230             TMAX                -33   \n",
      "5666        UK000047811  18641230             TMAX                 73   \n",
      "5667        ASN00090015  18641230             TMAX                211   \n",
      "5668        UK000056225  18641230             TMAX                 17   \n",
      "5669        SZ000006717  18641230             TMAX                -40   \n",
      "5670        ITE00100550  18641231             TMAX                 33   \n",
      "5671        ASN00086071  18641231             TMAX                234   \n",
      "5672        CA006158350  18641231             TMAX                -61   \n",
      "5673        EIE00101859  18641231             TMAX                 72   \n",
      "5674        BE000006447  18641231             TMAX                -16   \n",
      "5675        AGE00135039  18641231             TMAX                145   \n",
      "5676        GME00125218  18641231             TMAX                -30   \n",
      "5677        EZE00100082  18641231             TMAX                -68   \n",
      "5678        ITE00100554  18641231             TMAX                 15   \n",
      "5679        ASN00066062  18641231             TMAX                223   \n",
      "5680        AU000005901  18641231             TMAX                -28   \n",
      "5681        GM000004204  18641231             TMAX                -34   \n",
      "5682        UK000047811  18641231             TMAX                 40   \n",
      "5683        ASN00090015  18641231             TMAX                239   \n",
      "5684        UK000056225  18641231             TMAX                 16   \n",
      "5685        SZ000006717  18641231             TMAX                -62   \n",
      "\n",
      "     Country Code   Datetime  month  Latitude  Longitude  Elevation State  \\\n",
      "0             ITE 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "1             ASN 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "2              CA 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "3             EIE 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "4              BE 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "5             GME 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "6             EZE 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "7             ITE 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "8             ASN 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "9              AU 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "10             GM 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "11             UK 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "12             UK 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "13             SZ 1864-01-01      1       NaN        NaN        NaN   NaN   \n",
      "14            ITE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "15            ASN 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "16             CA 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "17            EIE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "18             BE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "19            AGE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "20            GME 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "21            EZE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "22            ITE 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "23            ASN 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "24             AU 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "25             GM 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "26             UK 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "27             UK 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "28             SZ 1864-01-02      1       NaN        NaN        NaN   NaN   \n",
      "29            ITE 1864-01-03      1       NaN        NaN        NaN   NaN   \n",
      "...           ...        ...    ...       ...        ...        ...   ...   \n",
      "5656           CA 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5657          EIE 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5658           BE 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5659          AGE 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5660          GME 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5661          EZE 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5662          ITE 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5663          ASN 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5664           AU 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5665           GM 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5666           UK 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5667          ASN 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5668           UK 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5669           SZ 1864-12-30     12       NaN        NaN        NaN   NaN   \n",
      "5670          ITE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5671          ASN 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5672           CA 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5673          EIE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5674           BE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5675          AGE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5676          GME 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5677          EZE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5678          ITE 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5679          ASN 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5680           AU 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5681           GM 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5682           UK 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5683          ASN 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5684           UK 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "5685           SZ 1864-12-31     12       NaN        NaN        NaN   NaN   \n",
      "\n",
      "     Name GSN Flag HCN/CRN Flag  WMO ID  \n",
      "0     NaN      NaN          NaN     NaN  \n",
      "1     NaN      NaN          NaN     NaN  \n",
      "2     NaN      NaN          NaN     NaN  \n",
      "3     NaN      NaN          NaN     NaN  \n",
      "4     NaN      NaN          NaN     NaN  \n",
      "5     NaN      NaN          NaN     NaN  \n",
      "6     NaN      NaN          NaN     NaN  \n",
      "7     NaN      NaN          NaN     NaN  \n",
      "8     NaN      NaN          NaN     NaN  \n",
      "9     NaN      NaN          NaN     NaN  \n",
      "10    NaN      NaN          NaN     NaN  \n",
      "11    NaN      NaN          NaN     NaN  \n",
      "12    NaN      NaN          NaN     NaN  \n",
      "13    NaN      NaN          NaN     NaN  \n",
      "14    NaN      NaN          NaN     NaN  \n",
      "15    NaN      NaN          NaN     NaN  \n",
      "16    NaN      NaN          NaN     NaN  \n",
      "17    NaN      NaN          NaN     NaN  \n",
      "18    NaN      NaN          NaN     NaN  \n",
      "19    NaN      NaN          NaN     NaN  \n",
      "20    NaN      NaN          NaN     NaN  \n",
      "21    NaN      NaN          NaN     NaN  \n",
      "22    NaN      NaN          NaN     NaN  \n",
      "23    NaN      NaN          NaN     NaN  \n",
      "24    NaN      NaN          NaN     NaN  \n",
      "25    NaN      NaN          NaN     NaN  \n",
      "26    NaN      NaN          NaN     NaN  \n",
      "27    NaN      NaN          NaN     NaN  \n",
      "28    NaN      NaN          NaN     NaN  \n",
      "29    NaN      NaN          NaN     NaN  \n",
      "...   ...      ...          ...     ...  \n",
      "5656  NaN      NaN          NaN     NaN  \n",
      "5657  NaN      NaN          NaN     NaN  \n",
      "5658  NaN      NaN          NaN     NaN  \n",
      "5659  NaN      NaN          NaN     NaN  \n",
      "5660  NaN      NaN          NaN     NaN  \n",
      "5661  NaN      NaN          NaN     NaN  \n",
      "5662  NaN      NaN          NaN     NaN  \n",
      "5663  NaN      NaN          NaN     NaN  \n",
      "5664  NaN      NaN          NaN     NaN  \n",
      "5665  NaN      NaN          NaN     NaN  \n",
      "5666  NaN      NaN          NaN     NaN  \n",
      "5667  NaN      NaN          NaN     NaN  \n",
      "5668  NaN      NaN          NaN     NaN  \n",
      "5669  NaN      NaN          NaN     NaN  \n",
      "5670  NaN      NaN          NaN     NaN  \n",
      "5671  NaN      NaN          NaN     NaN  \n",
      "5672  NaN      NaN          NaN     NaN  \n",
      "5673  NaN      NaN          NaN     NaN  \n",
      "5674  NaN      NaN          NaN     NaN  \n",
      "5675  NaN      NaN          NaN     NaN  \n",
      "5676  NaN      NaN          NaN     NaN  \n",
      "5677  NaN      NaN          NaN     NaN  \n",
      "5678  NaN      NaN          NaN     NaN  \n",
      "5679  NaN      NaN          NaN     NaN  \n",
      "5680  NaN      NaN          NaN     NaN  \n",
      "5681  NaN      NaN          NaN     NaN  \n",
      "5682  NaN      NaN          NaN     NaN  \n",
      "5683  NaN      NaN          NaN     NaN  \n",
      "5684  NaN      NaN          NaN     NaN  \n",
      "5685  NaN      NaN          NaN     NaN  \n",
      "\n",
      "[5686 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "df_merged = df_merged.rename(columns = {0: 'Station Identifier', 1: 'Latitude', 2: 'Longitude', 3: 'Elevation', 4: 'State',\\\n",
    "                                        5: 'Name', 6: 'GSN Flag', 7: 'HCN/CRN Flag', 8: 'WMO ID'})\n",
    "answer_73 = pd.merge(df_csvloop2, df_merged, how = 'left', on = 'Station Identifier')\n",
    "\n",
    "print(answer_73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70f00d284ec3a305a9c49ac27978f424",
     "grade": true,
     "grade_id": "problem_73_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_73.shape == (5686, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "71047da6e2e34b2c66bbddb6e2dba62a",
     "grade": false,
     "grade_id": "cell-01cb09ebc1fe07d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 8\n",
    "\n",
    "> **Ex. 8.1.2.:** Use the `request` module to collect the first page of job postings.\n",
    ">\n",
    "> Store the response.json() object in a new variable called `answer_81`.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3675e9996445e60c9c55871bbf9656b5",
     "grade": false,
     "grade_id": "problem_81",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.2]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a870431a18f53296a6d3676efb0e5ffc",
     "grade": true,
     "grade_id": "problem_81_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert sorted(answer_81.keys()) == sorted(['Expression', 'Facets', 'JobPositionPostings', 'TotalResultCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe4949db01017cc8618490263967fe1b",
     "grade": false,
     "grade_id": "cell-6f74cab704e1fb70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 8.1.3.:** Store the 'TotalResultCount' value for later use. Also create a dataframe from the 'JobPositionPostings' field in the json. Name this dataframe `answer_82`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b554f0f47fb8dacb41d1517d1afc42a3",
     "grade": false,
     "grade_id": "problem_82",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.3]\n",
    "# answer_82 = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a353e060971a565cb33498db98d2aaa5",
     "grade": true,
     "grade_id": "problem_82_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer_82.shape == (20,44)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
