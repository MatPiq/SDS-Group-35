{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model linear regression, Lasso, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the data\n",
    "\n",
    "X = df.loc[ : , ['sentiment', 'repo change']] # explanatory variables\n",
    "\n",
    "y = df.loc[ : , ['stock market index']] # value to be preicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-98e6aeb5b913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#split into training and test, random state to get the same output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m pipe_preproc = make_pipeline(PolynomialFeatures(), \n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0) #split into training and test, random state to get the same output.\n",
    "\n",
    "pipe_lr = make_pipeline(PolynomialFeatures(include_bias = False),  #including/excluding the bias depends on our choice\n",
    "                        StandardScaler(),\n",
    "                        LinearRegression())\n",
    "\n",
    "test_mse = []\n",
    "train_mse = []\n",
    "parameters = []\n",
    "degrees = range(3)\n",
    "\n",
    "for p in degrees:\n",
    "    X_train_p = pipe_lr.fit_transform(X_train) # polynomial expansion and transformation of the data\n",
    "    X_test_p = pipe_lr.transform(X_test) # transform takes the test data and rescles it\n",
    "    reg = fit(X_train_p, y_train) \n",
    "    train_mse += [mse(reg.predict(X_train_p),y_train)] \n",
    "    test_mse += [mse(reg.predict(X_test_p),y_test)]     \n",
    "    parameters.append(reg.coef_)\n",
    "    \n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and split development set into validation and training\n",
    "\n",
    "# splitting into development (2/3) and test data (1/3)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
    "\n",
    "# splitting development into train (1/3) and validation (1/3), so divide by half!\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso model: we remove the bias, i.e. beta_0 and we estimate a parameter that improves the prediction\n",
    "\n",
    "perform = []\n",
    "lambdas = np.logspace(-4, 4, 20)\n",
    "for lambda_ in lambdas:\n",
    "    pipe_lasso = make_pipeline(PolynomialFeatures(include_bias=False), #multiple pipelines\n",
    "                               StandardScaler(),\n",
    "                               Lasso(alpha=lambda_, random_state=1))\n",
    "    pipe_lasso.fit(X_train, y_train) # fit on training data\n",
    "    y_pred = pipe_lasso.predict(X_val) # predict performance on validation set\n",
    "    perform.append(mse(y_pred, y_val)) # saved as a series of lambdas\n",
    "    \n",
    "hyperparam_perform = pd.Series(perform,index=lambdas)\n",
    "hyperparam_perform.nsmallest(1) # select the smallest hyperparameter\n",
    "\n",
    "optimal = hyperparam_perform.nsmallest(1)    \n",
    "print('Optimal alpha:', optimal.index[0])\n",
    "print('Validation MSE: %.3f' % optimal.values[0])\n",
    "\n",
    "# insert optimal  lambda in new model\n",
    "\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                           StandardScaler(),\n",
    "                           Lasso(alpha=optimal.index[0])) # new value..., alpha is lambda!\n",
    "# fit new model on all development data, fit = estimate!\n",
    "pipe_lasso.fit(X_dev,y_dev)\n",
    "# compare model performance on test data\n",
    "print('Lasso', round(mse(pipe_lasso.predict(X_test),y_test), 3)) # smaller mse, better performance than linear model\n",
    "print('LinReg', round(mse(pipe_lr.predict(X_test),y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other evaluation method\n",
    "# CROSS VALIDATION\n",
    "\n",
    "# instead of using on epart of the data, we use all the data, rotating validation and training sets\n",
    "# 10% testing, 90% training. cross-validation changes the 10% share. \n",
    "\n",
    "# leave one out corss validation\n",
    "# take each obs, all other data used to make a model and that one used to test. most robust approach.\n",
    "\n",
    "# K-fold method: data divided into k bins, k-1 used to train the (entire) data.\n",
    "\n",
    "# K as large as possible (10 bins usually). this does not cause leakage because it is not the same model over and over.\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds = KFold(n_splits=10) # number of bins\n",
    "folds = list(kfolds.split(X_dev, y_dev))\n",
    "# outer loop: lambdas, for each of the lambda\n",
    "mseCV = []\n",
    "for lambda_ in lambdas:    \n",
    "    # inner loop: folds for each of the splits, we want to make the model\n",
    "    mseCV_ = []    \n",
    "    for train_idx, val_idx in folds:        \n",
    "        # train model and compute MSE on test fold\n",
    "        pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(alpha=lambda_, random_state=1))            \n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx] \n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx] \n",
    "        pipe_lassoCV.fit(X_train, y_train)     # fit the model on train data    \n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val))    # append on validation sample and store result\n",
    "        \n",
    "    # store result    \n",
    "    mseCV.append(mseCV_) # list of lists, becomes a dataframe next\n",
    "    \n",
    "# convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambdaCV.head(3))\n",
    "\n",
    "print(lambdaCV.mean(axis =1))\n",
    "\n",
    "# choose optimal hyperparameters \n",
    "optimal_lambda = lambdaCV.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# retrain model using optimal hyperparameters\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                             StandardScaler(),\n",
    "                             Lasso(alpha=optimal_lambda.index[0], random_state=1))\n",
    "pipe_lassoCV.fit(X_dev,y_dev) # fit the pipeline\n",
    "\n",
    "# compare performance\n",
    "models = {'Lasso': pipe_lasso, 'Lasso CV': pipe_lassoCV, 'LinReg': pipe_lr}\n",
    "for name, model in models.items():\n",
    "    score = mse(model.predict(X_test),y_test)\n",
    "    print(name, round(score, 2))\n",
    "\n",
    "# we see that lambda and lambda cross validation are just the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_lasso,\n",
    "                     X=X_train,\n",
    "                     y=y_train,\n",
    "                     param_name='lasso__alpha',\n",
    "                     param_range=lambdas,\n",
    "                     scoring='neg_mean_squared_error',                 \n",
    "                     cv=3)\n",
    "\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Validation':-test_scores.mean(axis=1),\n",
    "                          'lambda':lambdas})\\\n",
    "              .set_index('lambda')   \n",
    "print(mse_score.Validation.nsmallest(1))\n",
    "\n",
    "mse_score.plot(logx=True, logy=True) # optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELASTIC NET\n",
    "# more than one hyperparameters (L1, L2, n of features for polynoimial expansion (also chosen, can be optimized))\n",
    "# grid search = search in multiple dimension.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "pipe_el = make_pipeline(PolynomialFeatures(include_bias=False), # same as before\n",
    "                        StandardScaler(),\n",
    "                        ElasticNet())\n",
    "gs = GridSearchCV(estimator=pipe_el, \n",
    "                  param_grid={'elasticnet__alpha':np.logspace(-4,4,10)*2, # two-dimesional grid 10*10 = 100 combinations\n",
    "                              'elasticnet__l1_ratio':np.linspace(0,1,10)},  # how much the regularization of the two parameters \n",
    "                                                                            # should be\n",
    "                  scoring='neg_mean_squared_error', # - MSE, minimizer!\n",
    "                  n_jobs=4, # parallelize :) make computation faster\n",
    "                  iid=False, # was giving a warning\n",
    "                  cv=10)\n",
    "\n",
    "models['ElasicNetCV'] = gs.fit(X_train, y_train)\n",
    "for name, model in models.items():\n",
    "    score = mse(model.predict(X_test),y_test) # finally using the test data.\n",
    "    print(name, round(score, 2))\n",
    "print()\n",
    "print('CV params:', gs.best_params_) # net outperformed the lassob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures for classification accuracy\n",
    "\n",
    "# accuracy = true/(true + false)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# recall = share of correct answers\n",
    "# precision = condition on own predictions (could be 100% even though the share of correct predictions is small)\n",
    "# high recall = low false positive rate\n",
    "\n",
    "# nested cross-validation \n",
    "# test the model 5 times (fig.), in the inner we make the validation to get optimal hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
